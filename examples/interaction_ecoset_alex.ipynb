{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import lucid_kietzmannlab.modelzoo.vision_models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import lucid_kietzmannlab.modelzoo.vision_models as models\n",
    "import lucid_kietzmannlab.optvis.objectives as objectives\n",
    "import lucid_kietzmannlab.optvis.render as render\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import interact, Dropdown, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vkapoor/miniconda3/envs/lucidenv/lib/python3.12/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "2024-06-07 20:23:51.729544: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/vkapoor/Downloads/training_seed_05/model.ckpt_epoch89\n",
      "Model loaded from: /Users/vkapoor/Downloads/training_seed_05/model.ckpt_epoch89\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_dir = \"/Users/vkapoor/Downloads/training_seed_05\"\n",
    "model_checkpoint = \"model.ckpt_epoch89\"\n",
    "model, graph, sess, logits, activations, weights, layer_shape_dict = models.load_ecoset_model_seeds(model_checkpoint_dir, model_checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: alexnet_v2/conv1/Conv2D, Shape: (None, 54, 54, 64)\n",
      "Layer: alexnet_v2/conv1/BiasAdd, Shape: (None, 54, 54, 64)\n",
      "Layer: alexnet_v2/conv1/Relu, Shape: (None, 54, 54, 64)\n",
      "Layer: alexnet_v2/pool1/MaxPool, Shape: (None, 26, 26, 64)\n",
      "Layer: alexnet_v2/conv2/Conv2D, Shape: (None, 26, 26, 192)\n",
      "Layer: alexnet_v2/conv2/BiasAdd, Shape: (None, 26, 26, 192)\n",
      "Layer: alexnet_v2/conv2/Relu, Shape: (None, 26, 26, 192)\n",
      "Layer: alexnet_v2/pool2/MaxPool, Shape: (None, 12, 12, 192)\n",
      "Layer: alexnet_v2/conv3/Conv2D, Shape: (None, 12, 12, 384)\n",
      "Layer: alexnet_v2/conv3/BiasAdd, Shape: (None, 12, 12, 384)\n",
      "Layer: alexnet_v2/conv3/Relu, Shape: (None, 12, 12, 384)\n",
      "Layer: alexnet_v2/conv4/Conv2D, Shape: (None, 12, 12, 384)\n",
      "Layer: alexnet_v2/conv4/BiasAdd, Shape: (None, 12, 12, 384)\n",
      "Layer: alexnet_v2/conv4/Relu, Shape: (None, 12, 12, 384)\n",
      "Layer: alexnet_v2/conv5/Conv2D, Shape: (None, 12, 12, 256)\n",
      "Layer: alexnet_v2/conv5/BiasAdd, Shape: (None, 12, 12, 256)\n",
      "Layer: alexnet_v2/conv5/Relu, Shape: (None, 12, 12, 256)\n",
      "Layer: alexnet_v2/pool5/MaxPool, Shape: (None, 5, 5, 256)\n",
      "Layer: alexnet_v2/fc6/Conv2D, Shape: (None, 1, 1, 4096)\n",
      "Layer: alexnet_v2/fc6/BiasAdd, Shape: (None, 1, 1, 4096)\n",
      "Layer: alexnet_v2/fc6/Relu, Shape: (None, 1, 1, 4096)\n",
      "Layer: alexnet_v2/dropout6/Identity, Shape: (None, 1, 1, 4096)\n",
      "Layer: alexnet_v2/fc7/Conv2D, Shape: (None, 1, 1, 4096)\n",
      "Layer: alexnet_v2/fc7/BiasAdd, Shape: (None, 1, 1, 4096)\n",
      "Layer: alexnet_v2/fc7/Relu, Shape: (None, 1, 1, 4096)\n",
      "Layer: alexnet_v2/dropout7/Identity, Shape: (None, 1, 1, 4096)\n",
      "Layer: alexnet_v2/fc8/Conv2D, Shape: (None, 1, 1, 1000)\n",
      "Layer: alexnet_v2/fc8/BiasAdd, Shape: (None, 1, 1, 1000)\n",
      "Layer: alexnet_v2/fc8/squeezed, Shape: (None, 1000)\n",
      "Layer: save/RestoreV2, Shape: <unknown>\n",
      "Layer: save/Identity, Shape: <unknown>\n",
      "Layer: save/Identity_1, Shape: <unknown>\n",
      "Layer: save/Identity_2, Shape: <unknown>\n",
      "Layer: save/Identity_3, Shape: <unknown>\n",
      "Layer: save/Identity_4, Shape: <unknown>\n",
      "Layer: save/Identity_5, Shape: <unknown>\n",
      "Layer: save/Identity_6, Shape: <unknown>\n",
      "Layer: save/Identity_7, Shape: <unknown>\n",
      "Layer: save/Identity_8, Shape: <unknown>\n",
      "Layer: save/Identity_9, Shape: <unknown>\n",
      "Layer: save/Identity_10, Shape: <unknown>\n",
      "Layer: save/Identity_11, Shape: <unknown>\n",
      "Layer: save/Identity_12, Shape: <unknown>\n",
      "Layer: save/Identity_13, Shape: <unknown>\n",
      "Layer: save/Identity_14, Shape: <unknown>\n",
      "Layer: save/Identity_15, Shape: <unknown>\n"
     ]
    }
   ],
   "source": [
    "for layer, shape in layer_shape_dict.items():\n",
    "    print(f'Layer: {layer}, Shape: {shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = lambda neuron: objectives.channel(*neuron)    \n",
    "def visualize(layer_name, channel):\n",
    "    # Check if the layer exists in the shape dictionary\n",
    "    if layer_name in layer_shape_dict:\n",
    "        # Check if the selected channel is within bounds\n",
    "        print(layer_shape_dict[layer_name])\n",
    "        max_channel = layer_shape_dict[layer_name][-1] - 1\n",
    "        if 0 <= channel <= max_channel:\n",
    "            #clear_output(wait=True)\n",
    "            # Render visualization for the selected layer and channel\n",
    "            #try:\n",
    "            _ = render.render_vis(model, C((layer_name, channel)), scope='alexnet_v2')\n",
    "            #except Exception:\n",
    "            #    print('No gradients for this layer')   \n",
    "\n",
    "def visualize_all():\n",
    "    # Check if the layer exists in the shape dictionary\n",
    "    layer_name = current_dropdown_value({'new': layer_dropdown.value})\n",
    "    print(layer_name)\n",
    "    if layer_name in layer_shape_dict:\n",
    "            # Check if the selected channel is within bounds\n",
    "            try:\n",
    "               image_channel = {} \n",
    "               for channel in tqdm(range(channel_slider.max)):    \n",
    "                  images = render.render_vis(model, C((layer_name, channel)), verbose = False)\n",
    "                  image_channel[channel] = images \n",
    "            except Exception:\n",
    "                print('No gradients for this layer')    \n",
    "    return image_channel   \n",
    "\n",
    "\n",
    "# Create dropdown menu for layer selection\n",
    "layer_dropdown = Dropdown(options=list(layer_shape_dict.keys()), description='Layer:')\n",
    "\n",
    "# Create slider for channel selection\n",
    "channel_slider = IntSlider(min=0, max=0, description='Channel:')\n",
    "        \n",
    "        \n",
    "def update_channel_slider(change):\n",
    "    layer_name = change.new\n",
    "    if layer_name in layer_shape_dict:\n",
    "        print(layer_name)\n",
    "        print(layer_shape_dict[layer_name])\n",
    "        max_channel = layer_shape_dict[layer_name][-1] - 1\n",
    "        channel_slider.max = max_channel\n",
    "        \n",
    "        \n",
    "def current_slider_value(*args):\n",
    "    return channel_slider.value\n",
    "\n",
    "\n",
    "def current_dropdown_value(change):\n",
    "    return change['new']\n",
    "\n",
    "\n",
    "channel_slider.observe(current_slider_value, names='value')\n",
    "layer_dropdown.observe(current_dropdown_value, names='value')\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dropdown.observe(update_channel_slider, names='value')\n",
    "\n",
    "# Create an interactive visualization\n",
    "interact(visualize, layer_name=layer_dropdown, channel=channel_slider)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['alexnet_v2/conv1/Conv2D', 'alexnet_v2/conv1/BiasAdd', 'alexnet_v2/conv1/Relu', 'alexnet_v2/pool1/MaxPool', 'alexnet_v2/conv2/Conv2D', 'alexnet_v2/conv2/BiasAdd', 'alexnet_v2/conv2/Relu', 'alexnet_v2/pool2/MaxPool', 'alexnet_v2/conv3/Conv2D', 'alexnet_v2/conv3/BiasAdd', 'alexnet_v2/conv3/Relu', 'alexnet_v2/conv4/Conv2D', 'alexnet_v2/conv4/BiasAdd', 'alexnet_v2/conv4/Relu', 'alexnet_v2/conv5/Conv2D', 'alexnet_v2/conv5/BiasAdd', 'alexnet_v2/conv5/Relu', 'alexnet_v2/pool5/MaxPool', 'alexnet_v2/fc6/Conv2D', 'alexnet_v2/fc6/BiasAdd', 'alexnet_v2/fc6/Relu', 'alexnet_v2/dropout6/Identity', 'alexnet_v2/fc7/Conv2D', 'alexnet_v2/fc7/BiasAdd', 'alexnet_v2/fc7/Relu', 'alexnet_v2/dropout7/Identity', 'alexnet_v2/fc8/Conv2D', 'alexnet_v2/fc8/BiasAdd', 'alexnet_v2/fc8/squeezed', 'save/RestoreV2', 'save/Identity', 'save/Identity_1', 'save/Identity_2', 'save/Identity_3', 'save/Identity_4', 'save/Identity_5', 'save/Identity_6', 'save/Identity_7', 'save/Identity_8', 'save/Identity_9', 'save/Identity_10', 'save/Identity_11', 'save/Identity_12', 'save/Identity_13', 'save/Identity_14', 'save/Identity_15'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_shape_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name_list = [layer_info[\"name\"] for layer_info in model.layers]\n",
    "layer_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'alexnet_v2/conv1/Conv2D:0' shape=(None, 54, 54, 64) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lname = 'alexnet_v2/conv1/Conv2D'\n",
    "model.get_tensor(lname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'alexnet_v2/conv1/weights:0' shape=(11, 11, 3, 64) dtype=float32>,\n",
       " <tf.Variable 'alexnet_v2/conv2/weights:0' shape=(5, 5, 64, 192) dtype=float32>,\n",
       " <tf.Variable 'alexnet_v2/conv3/weights:0' shape=(3, 3, 192, 384) dtype=float32>,\n",
       " <tf.Variable 'alexnet_v2/conv4/weights:0' shape=(3, 3, 384, 384) dtype=float32>,\n",
       " <tf.Variable 'alexnet_v2/conv5/weights:0' shape=(3, 3, 384, 256) dtype=float32>,\n",
       " <tf.Variable 'alexnet_v2/fc6/weights:0' shape=(5, 5, 256, 4096) dtype=float32>,\n",
       " <tf.Variable 'alexnet_v2/fc7/weights:0' shape=(1, 1, 4096, 4096) dtype=float32>,\n",
       " <tf.Variable 'alexnet_v2/fc8/weights:0' shape=(1, 1, 4096, 1000) dtype=float32>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 20:25:53.267683: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable alexnet_v2/conv1/weights. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Container localhost does not exist. (Could not find resource: localhost/alexnet_v2/conv1/weights)\n",
      "\t [[{{node import/alexnet_v2/conv1/Conv2D/ReadVariableOp}}]]\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "in user code:\n\n    File \"/Users/vkapoor/python_workspace/lucid-kietzmannlab/src/lucid_kietzmannlab/modelzoo/vision_base.py\", line 179, in get_activations_iter  *\n        acts = t_layer.eval({t_img: imgs})\n\n    FailedPreconditionError: Graph execution error:\n    \n    Detected at node 'import/alexnet_v2/conv1/Conv2D/ReadVariableOp' defined at (most recent call last):\n    Node: 'import/alexnet_v2/conv1/Conv2D/ReadVariableOp'\n    Could not find variable alexnet_v2/conv1/weights. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Container localhost does not exist. (Could not find resource: localhost/alexnet_v2/conv1/weights)\n    \t [[{{node import/alexnet_v2/conv1/Conv2D/ReadVariableOp}}]]\n    \n    Original stack trace for 'import/alexnet_v2/conv1/Conv2D/ReadVariableOp':\n    \n\n\nOriginal stack trace for 'import/alexnet_v2/conv1/Conv2D/ReadVariableOp':\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      6\u001b[0m random_tensor_normal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\n\u001b[1;32m      7\u001b[0m     size\u001b[38;5;241m=\u001b[39m[batch_size, height, width, channels]\n\u001b[1;32m      8\u001b[0m     \n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_tensor_normal\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_workspace/lucid-kietzmannlab/src/lucid_kietzmannlab/modelzoo/vision_base.py:624\u001b[0m, in \u001b[0;36mModel.get_activations\u001b[0;34m(self, layer, examples, batch_size, dtype, ind_shape, center_only)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_activations\u001b[39m(\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    588\u001b[0m     layer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m     center_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    594\u001b[0m ):\n\u001b[1;32m    595\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect center activtions of a layer over an n-dimensional array of images.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m    Note: this is mostly intended for large synthetic families of images, where\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m      A numpy array of shape [ind1, ind2, ..., layer_channels]\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_activations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mind_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mind_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_workspace/lucid-kietzmannlab/src/lucid_kietzmannlab/modelzoo/vision_base.py:272\u001b[0m, in \u001b[0;36m_get_activations\u001b[0;34m(model, layer, examples, batch_size, dtype, ind_shape, center_only)\u001b[0m\n\u001b[1;32m    266\u001b[0m examples_enumerated \u001b[38;5;241m=\u001b[39m recursive_enumerate_nd(\n\u001b[1;32m    267\u001b[0m     examples,\n\u001b[1;32m    268\u001b[0m     stop_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    269\u001b[0m )\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Get responses\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_activations_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples_enumerated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mind_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mind_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lucidenv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/lucidenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: in user code:\n\n    File \"/Users/vkapoor/python_workspace/lucid-kietzmannlab/src/lucid_kietzmannlab/modelzoo/vision_base.py\", line 179, in get_activations_iter  *\n        acts = t_layer.eval({t_img: imgs})\n\n    FailedPreconditionError: Graph execution error:\n    \n    Detected at node 'import/alexnet_v2/conv1/Conv2D/ReadVariableOp' defined at (most recent call last):\n    Node: 'import/alexnet_v2/conv1/Conv2D/ReadVariableOp'\n    Could not find variable alexnet_v2/conv1/weights. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Container localhost does not exist. (Could not find resource: localhost/alexnet_v2/conv1/weights)\n    \t [[{{node import/alexnet_v2/conv1/Conv2D/ReadVariableOp}}]]\n    \n    Original stack trace for 'import/alexnet_v2/conv1/Conv2D/ReadVariableOp':\n    \n\n\nOriginal stack trace for 'import/alexnet_v2/conv1/Conv2D/ReadVariableOp':\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "batch_size = 1\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "random_tensor_normal = np.random.normal(\n",
    "    size=[batch_size, height, width, channels]\n",
    "    \n",
    ")\n",
    "\n",
    "model.get_activations(lname, random_tensor_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lucidenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
